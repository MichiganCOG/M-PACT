#!/bin/bash -f
#PBS -N TFRecords_1gpu_C3D_SR_Train_HMDB51_Test_HMDB51_Alpha_0_2     # Name of the job
#PBS -j oe                             # Join output and error in single file
#PBS -l walltime=500:00:00             # walltime requested
#PBS -l mem=10Gb                       # memory requested
#PBS -l nodes=macula:ppn=5:gpus=1    # nodes requested: Processor per node: gpus requested
#PBS -M erichof@umich.edu      #  send email to user@umich.edu
#PBS -m abe                     # send email on abort, begin and end
#PBS -V                         # Pass current environment variables to the script

##########################################
#                                        #
#   Output some useful job information.  #
#                                        #
##########################################

# Print all the environment variables exported
# PBS_JOBNAME 	User specified jobname
# PBS_ARRAYID 	Zero-based value of job array index for this job (in version 2.2.0 and later)
# PBS_GPUFILE
#
# Line-delimited list of GPUs allocated to the job located in TORQUE_HOME/aux/jobidgpu. Each line follows the following format:
#
# <host>-gpu<number>
#
# For example, myhost-gpu1.
# PBS_JOBNAME 	User specified jobname
# PBS_ARRAYID 	Zero-based value of job array index for this job (in version 2.2.0 and later)
# PBS_GPUFILE
#
# Line-delimited list of GPUs allocated to the job located in TORQUE_HOME/aux/jobidgpu. Each line follows the following format:
#
# <host>-gpu<number>
#
# For example, myhost-gpu1.
# PBS_O_WORKDIR 	Job's submission directory
# PBS_ENVIRONMENT 	N/A
# PBS_TASKNUM 	    Number of tasks requested
# PBS_O_HOME 	    Home directory of submitting user
# PBS_MOMPORT 	    Active port for MOM daemon
# PBS_O_LOGNAME 	Name of submitting user
# PBS_O_LANG 	    Language variable for job
# PBS_JOBCOOKIE 	Job cookie
# PBS_JOBID 	    Unique pbs job id
# PBS_NODENUM 	    Node offset number
# PBS_NUM_NODES 	Number of nodes allocated to the job
# PBS_NUM_PPN 	    Number of procs per node allocated to the job
# PBS_O_SHELL 	    Script shell
# PBS_O_HOST 	    Host on which job script is currently running
# PBS_QUEUE 	    Job queue
# PBS_NODEFILE 	    File containing line delimited list of nodes allocated to the job
# PBS_NP 	        Number of execution slots (cores) for the job
# PBS_O_PATH 	    Path variable used to locate executables within job script

echo ------------------------------------------------------
echo -n 'Job is running on node '; cat $PBS_NODEFILE
echo ------------------------------------------------------

# _O_ means originating
echo PBS_ARRAYID     : $PBS_ARRAYID
echo PBS_ENVIRONMENT : $PBS_ENVIRONMENT
echo PBS_GPUFILE     : $PBS_GPUFILE
echo PBS_JOBCOOKIE   : $PBS_JOBCOOKIE
echo PBS_JOBID 	     : $PBS_JOBID
echo PBS_JOBNAME     : $PBS_JOBNAME
echo PBS_MOMPORT     : $PBS_MOMPORT
echo PBS_NODEFILE    : $PBS_NODEFILE
echo PBS_NODENUM     : $PBS_NODENUM
echo PBS_NP 	     : $PBS_NP
echo PBS_NUM_NODES   : $PBS_NUM_NODES
echo PBS_NUM_PPN     : $PBS_NUM_PPN
echo PBS_O_HOME      : $PBS_O_HOME
echo PBS_O_HOST      : $PBS_O_HOST
echo PBS_O_LANG      : $PBS_O_LANG
echo PBS_O_LOGNAME   : $PBS_O_LOGNAME
echo PBS_O_PATH      : $PBS_O_PATH
echo PBS_O_SHELL     : $PBS_O_SHELL
echo PBS_O_WORKDIR   : $PBS_O_WORKDIR
echo PBS_QUEUE 	     : $PBS_QUEUE
echo PBS_TASKNUM     : $PBS_TASKNUM

echo The contents of gpu file are:
cat $PBS_GPUFILE
echo ----------End of GPU file------

IFS=$'\n' allocatedgpus=($(sed -e 's/.*-gpu\([0-9]\+\)/\1/' $PBS_GPUFILE))
echo I got allocated the following gpus
for g in "${allocatedgpus[@]}"; do
    echo "gpu: $g"
done

echo PBS: qsub is running on $PBS_O_HOST
echo PBS: originating queue is $PBS_O_QUEUE
echo PBS: executing queue is $PBS_QUEUE
echo PBS: working directory is $PBS_O_WORKDIR
echo PBS: execution mode is $PBS_ENVIRONMENT
echo PBS: job identifier is $PBS_JOBID
echo PBS: job name is $PBS_JOBNAME
echo PBS: node file is $PBS_NODEFILE
echo PBS: current home directory is $PBS_O_HOME
echo PBS: PATH = $PBS_O_PATH
echo ------------------------------------------------------

echo "start"
hostname
nvidia-smi
export CUDA_VISIBLE_DEVICES="${allocatedgpus[0]}"
module load numpy cuda cudnn gflags tensorflow/1.2.1 opencv/2.4.13/16.04

cd /z/home/erichof/Madan_TFRecords/tf-activity-recognition-framework/
echo "Train C3D: HMDB51"

python train_test_TFRecords_multigpu_model.py \
--model c3d_sr \
--dataset HMDB51 \
--loadedDataset HMDB51 \
--train 0 \
--load 1 \
--inputDims 16 \
--outputDims 51 \
--seqLength 1 \
--size 112  \
--expName c3d_sr_HMDB51 \
--numClips 1 \
--clipLength 50 \
--clipOffset random \
--numVids 1530 \
--split 1 \
--baseDataPath /z/dat \
--fName testlist \
--verbose 1 \
--inputAlpha 0.2 \
--metricsDir input_alpha_0_2



echo "end"
